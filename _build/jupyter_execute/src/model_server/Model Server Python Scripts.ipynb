{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'model_server'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_90361/2092780140.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmodel_server\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmsg\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInferenceResults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetsource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mInferenceResults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'model_server'"
     ]
    }
   ],
   "source": [
    "from model_server.msg import InferenceResults\n",
    "lines = inspect.getsource(InferenceResults)\n",
    "print(lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class ImageRequest(object):\n",
      "  _type          = 'model_server/ImageRequest'\n",
      "  _md5sum = 'fba05cbe092f5fd7d13b6a12503ed6ac'\n",
      "  _request_class  = ImageRequestRequest\n",
      "  _response_class = ImageRequestResponse\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from model_server.srv import ImageRequest, ImageRequestResponse\n",
    "lines = inspect.getsource(ImageRequest)\n",
    "print(lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class ImageRequestResponse(genpy.Message):\n",
      "  _md5sum = \"17b637004ab60f377423869fa9425b3c\"\n",
      "  _type = \"model_server/ImageRequestResponse\"\n",
      "  _has_header = False  # flag to mark the presence of a Header object\n",
      "  _full_text = \"\"\"std_msgs/Bool   status\n",
      "\n",
      "\n",
      "================================================================================\n",
      "MSG: std_msgs/Bool\n",
      "bool data\"\"\"\n",
      "  __slots__ = ['status']\n",
      "  _slot_types = ['std_msgs/Bool']\n",
      "\n",
      "  def __init__(self, *args, **kwds):\n",
      "    \"\"\"\n",
      "    Constructor. Any message fields that are implicitly/explicitly\n",
      "    set to None will be assigned a default value. The recommend\n",
      "    use is keyword arguments as this is more robust to future message\n",
      "    changes.  You cannot mix in-order arguments and keyword arguments.\n",
      "\n",
      "    The available fields are:\n",
      "       status\n",
      "\n",
      "    :param args: complete set of field values, in .msg order\n",
      "    :param kwds: use keyword arguments corresponding to message field names\n",
      "    to set specific fields.\n",
      "    \"\"\"\n",
      "    if args or kwds:\n",
      "      super(ImageRequestResponse, self).__init__(*args, **kwds)\n",
      "      # message fields cannot be None, assign default values for those that are\n",
      "      if self.status is None:\n",
      "        self.status = std_msgs.msg.Bool()\n",
      "    else:\n",
      "      self.status = std_msgs.msg.Bool()\n",
      "\n",
      "  def _get_types(self):\n",
      "    \"\"\"\n",
      "    internal API method\n",
      "    \"\"\"\n",
      "    return self._slot_types\n",
      "\n",
      "  def serialize(self, buff):\n",
      "    \"\"\"\n",
      "    serialize message into buffer\n",
      "    :param buff: buffer, ``StringIO``\n",
      "    \"\"\"\n",
      "    try:\n",
      "      _x = self.status.data\n",
      "      buff.write(_get_struct_B().pack(_x))\n",
      "    except struct.error as se: self._check_types(struct.error(\"%s: '%s' when writing '%s'\" % (type(se), str(se), str(locals().get('_x', self)))))\n",
      "    except TypeError as te: self._check_types(ValueError(\"%s: '%s' when writing '%s'\" % (type(te), str(te), str(locals().get('_x', self)))))\n",
      "\n",
      "  def deserialize(self, str):\n",
      "    \"\"\"\n",
      "    unpack serialized message in str into this message instance\n",
      "    :param str: byte array of serialized message, ``str``\n",
      "    \"\"\"\n",
      "    if python3:\n",
      "      codecs.lookup_error(\"rosmsg\").msg_type = self._type\n",
      "    try:\n",
      "      if self.status is None:\n",
      "        self.status = std_msgs.msg.Bool()\n",
      "      end = 0\n",
      "      start = end\n",
      "      end += 1\n",
      "      (self.status.data,) = _get_struct_B().unpack(str[start:end])\n",
      "      self.status.data = bool(self.status.data)\n",
      "      return self\n",
      "    except struct.error as e:\n",
      "      raise genpy.DeserializationError(e)  # most likely buffer underfill\n",
      "\n",
      "\n",
      "  def serialize_numpy(self, buff, numpy):\n",
      "    \"\"\"\n",
      "    serialize message with numpy array types into buffer\n",
      "    :param buff: buffer, ``StringIO``\n",
      "    :param numpy: numpy python module\n",
      "    \"\"\"\n",
      "    try:\n",
      "      _x = self.status.data\n",
      "      buff.write(_get_struct_B().pack(_x))\n",
      "    except struct.error as se: self._check_types(struct.error(\"%s: '%s' when writing '%s'\" % (type(se), str(se), str(locals().get('_x', self)))))\n",
      "    except TypeError as te: self._check_types(ValueError(\"%s: '%s' when writing '%s'\" % (type(te), str(te), str(locals().get('_x', self)))))\n",
      "\n",
      "  def deserialize_numpy(self, str, numpy):\n",
      "    \"\"\"\n",
      "    unpack serialized message in str into this message instance using numpy for array types\n",
      "    :param str: byte array of serialized message, ``str``\n",
      "    :param numpy: numpy python module\n",
      "    \"\"\"\n",
      "    if python3:\n",
      "      codecs.lookup_error(\"rosmsg\").msg_type = self._type\n",
      "    try:\n",
      "      if self.status is None:\n",
      "        self.status = std_msgs.msg.Bool()\n",
      "      end = 0\n",
      "      start = end\n",
      "      end += 1\n",
      "      (self.status.data,) = _get_struct_B().unpack(str[start:end])\n",
      "      self.status.data = bool(self.status.data)\n",
      "      return self\n",
      "    except struct.error as e:\n",
      "      raise genpy.DeserializationError(e)  # most likely buffer underfill\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lines = inspect.getsource(ImageRequestResponse)\n",
    "print(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class ModelRequest(object):\n",
      "  _type          = 'model_server/ModelRequest'\n",
      "  _md5sum = '84e67d26165d3d65758a56814ed143ba'\n",
      "  _request_class  = ModelRequestRequest\n",
      "  _response_class = ModelRequestResponse\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from model_server.srv import ModelRequest, ModelRequestResponse\n",
    "lines = inspect.getsource(ModelRequest)\n",
    "print(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class ModelRequestResponse(genpy.Message):\n",
      "  _md5sum = \"6af076944b82152d7ca8f015f76ac624\"\n",
      "  _type = \"model_server/ModelRequestResponse\"\n",
      "  _has_header = False  # flag to mark the presence of a Header object\n",
      "  _full_text = \"\"\"InferenceResults    results\n",
      "\n",
      "\n",
      "================================================================================\n",
      "MSG: model_server/InferenceResults\n",
      "int32[] structure\n",
      "float32[]   inferences\n",
      "\"\"\"\n",
      "  __slots__ = ['results']\n",
      "  _slot_types = ['model_server/InferenceResults']\n",
      "\n",
      "  def __init__(self, *args, **kwds):\n",
      "    \"\"\"\n",
      "    Constructor. Any message fields that are implicitly/explicitly\n",
      "    set to None will be assigned a default value. The recommend\n",
      "    use is keyword arguments as this is more robust to future message\n",
      "    changes.  You cannot mix in-order arguments and keyword arguments.\n",
      "\n",
      "    The available fields are:\n",
      "       results\n",
      "\n",
      "    :param args: complete set of field values, in .msg order\n",
      "    :param kwds: use keyword arguments corresponding to message field names\n",
      "    to set specific fields.\n",
      "    \"\"\"\n",
      "    if args or kwds:\n",
      "      super(ModelRequestResponse, self).__init__(*args, **kwds)\n",
      "      # message fields cannot be None, assign default values for those that are\n",
      "      if self.results is None:\n",
      "        self.results = model_server.msg.InferenceResults()\n",
      "    else:\n",
      "      self.results = model_server.msg.InferenceResults()\n",
      "\n",
      "  def _get_types(self):\n",
      "    \"\"\"\n",
      "    internal API method\n",
      "    \"\"\"\n",
      "    return self._slot_types\n",
      "\n",
      "  def serialize(self, buff):\n",
      "    \"\"\"\n",
      "    serialize message into buffer\n",
      "    :param buff: buffer, ``StringIO``\n",
      "    \"\"\"\n",
      "    try:\n",
      "      length = len(self.results.structure)\n",
      "      buff.write(_struct_I.pack(length))\n",
      "      pattern = '<%si'%length\n",
      "      buff.write(struct.Struct(pattern).pack(*self.results.structure))\n",
      "      length = len(self.results.inferences)\n",
      "      buff.write(_struct_I.pack(length))\n",
      "      pattern = '<%sf'%length\n",
      "      buff.write(struct.Struct(pattern).pack(*self.results.inferences))\n",
      "    except struct.error as se: self._check_types(struct.error(\"%s: '%s' when writing '%s'\" % (type(se), str(se), str(locals().get('_x', self)))))\n",
      "    except TypeError as te: self._check_types(ValueError(\"%s: '%s' when writing '%s'\" % (type(te), str(te), str(locals().get('_x', self)))))\n",
      "\n",
      "  def deserialize(self, str):\n",
      "    \"\"\"\n",
      "    unpack serialized message in str into this message instance\n",
      "    :param str: byte array of serialized message, ``str``\n",
      "    \"\"\"\n",
      "    if python3:\n",
      "      codecs.lookup_error(\"rosmsg\").msg_type = self._type\n",
      "    try:\n",
      "      if self.results is None:\n",
      "        self.results = model_server.msg.InferenceResults()\n",
      "      end = 0\n",
      "      start = end\n",
      "      end += 4\n",
      "      (length,) = _struct_I.unpack(str[start:end])\n",
      "      pattern = '<%si'%length\n",
      "      start = end\n",
      "      s = struct.Struct(pattern)\n",
      "      end += s.size\n",
      "      self.results.structure = s.unpack(str[start:end])\n",
      "      start = end\n",
      "      end += 4\n",
      "      (length,) = _struct_I.unpack(str[start:end])\n",
      "      pattern = '<%sf'%length\n",
      "      start = end\n",
      "      s = struct.Struct(pattern)\n",
      "      end += s.size\n",
      "      self.results.inferences = s.unpack(str[start:end])\n",
      "      return self\n",
      "    except struct.error as e:\n",
      "      raise genpy.DeserializationError(e)  # most likely buffer underfill\n",
      "\n",
      "\n",
      "  def serialize_numpy(self, buff, numpy):\n",
      "    \"\"\"\n",
      "    serialize message with numpy array types into buffer\n",
      "    :param buff: buffer, ``StringIO``\n",
      "    :param numpy: numpy python module\n",
      "    \"\"\"\n",
      "    try:\n",
      "      length = len(self.results.structure)\n",
      "      buff.write(_struct_I.pack(length))\n",
      "      pattern = '<%si'%length\n",
      "      buff.write(self.results.structure.tostring())\n",
      "      length = len(self.results.inferences)\n",
      "      buff.write(_struct_I.pack(length))\n",
      "      pattern = '<%sf'%length\n",
      "      buff.write(self.results.inferences.tostring())\n",
      "    except struct.error as se: self._check_types(struct.error(\"%s: '%s' when writing '%s'\" % (type(se), str(se), str(locals().get('_x', self)))))\n",
      "    except TypeError as te: self._check_types(ValueError(\"%s: '%s' when writing '%s'\" % (type(te), str(te), str(locals().get('_x', self)))))\n",
      "\n",
      "  def deserialize_numpy(self, str, numpy):\n",
      "    \"\"\"\n",
      "    unpack serialized message in str into this message instance using numpy for array types\n",
      "    :param str: byte array of serialized message, ``str``\n",
      "    :param numpy: numpy python module\n",
      "    \"\"\"\n",
      "    if python3:\n",
      "      codecs.lookup_error(\"rosmsg\").msg_type = self._type\n",
      "    try:\n",
      "      if self.results is None:\n",
      "        self.results = model_server.msg.InferenceResults()\n",
      "      end = 0\n",
      "      start = end\n",
      "      end += 4\n",
      "      (length,) = _struct_I.unpack(str[start:end])\n",
      "      pattern = '<%si'%length\n",
      "      start = end\n",
      "      s = struct.Struct(pattern)\n",
      "      end += s.size\n",
      "      self.results.structure = numpy.frombuffer(str[start:end], dtype=numpy.int32, count=length)\n",
      "      start = end\n",
      "      end += 4\n",
      "      (length,) = _struct_I.unpack(str[start:end])\n",
      "      pattern = '<%sf'%length\n",
      "      start = end\n",
      "      s = struct.Struct(pattern)\n",
      "      end += s.size\n",
      "      self.results.inferences = numpy.frombuffer(str[start:end], dtype=numpy.float32, count=length)\n",
      "      return self\n",
      "    except struct.error as e:\n",
      "      raise genpy.DeserializationError(e)  # most likely buffer underfill\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lines = inspect.getsource(ModelRequestResponse)\n",
    "print(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class ViperModel(object):\n",
      "    # A ViperModel contains the location of the model architecture\n",
      "    # and the model weights which are stored in the node package.\n",
      "    \n",
      "    def __init__(self, package_name, model_xml, weights_bin):\n",
      "        self.pkg = package_name\n",
      "        self.model = model_xml\n",
      "        self.weights = weights_bin\n",
      "        self.setup_model()\n",
      "        \n",
      "    def setup_model(self):\n",
      "        self.dir = roslib.packages.get_pkg_dir(self.pkg)\n",
      "        self.location = os.path.join(\n",
      "            self.dir, \n",
      "            self.model\n",
      "            )\n",
      "        self.weights = os.path.join(\n",
      "            self.dir, \n",
      "            self.weights\n",
      "            )\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from model_server import ViperModel\n",
    "lines = inspect.getsource(ViperModel)\n",
    "print(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class NeuralNetworkLoader(object):\n",
      "    \"\"\"\n",
      "    \n",
      "    A NeuralNetworkLoader is a class object which loads a pretrained\n",
      "    model (architecture and weights) onto a initialized OpenVino\n",
      "    inference engine.\n",
      "    \n",
      "    Keyword Arguments:\n",
      "    \n",
      "    ie -- an Inference Engine instance set up in the parent node which \n",
      "    we will load this model to.\n",
      "    \n",
      "    ViperModel -- a instance of class ViperModel which contains the \n",
      "    models weights and the structure of the neural network.\n",
      "    \n",
      "    device -- the inference device to be used to predict on (i.e., \n",
      "    \"MYRIAD\", CPU, GPU, etc.)\n",
      "    \n",
      "    model_tag -- a three letter abbreviation used by the VIPER Logger\n",
      "    module which identifies log messages as originating from within this\n",
      "    modules code.\n",
      "    \n",
      "    model_name -- a logger attribute which identifies this model.\n",
      "    \n",
      "    \"\"\"\n",
      "    \n",
      "    def __init__(self, \n",
      "                ie: IECore, \n",
      "                viper_model: ViperModel,\n",
      "                device: str,\n",
      "                model_tag: str = \"M..\",\n",
      "                model_name: str = \"Model\",\n",
      "                *args,\n",
      "                **kwargs):\n",
      "        \n",
      "        # Creates our helper tools from our Viper Toolkkit such as \n",
      "        # the parameter manager, our log manager, and our timer.\n",
      "        self.setup_parameters(\n",
      "            model_name = model_name, \n",
      "            model_tag = model_tag)\n",
      "        \n",
      "        # Prepare this model for loading\n",
      "        self.setup_inference_engine(\n",
      "            ie = ie, \n",
      "            viper_model = viper_model, \n",
      "            device = device)\n",
      "\n",
      "        # Load the read network onto the initialized device.\n",
      "        self.load_inference_engine(device=device, ie = ie)\n",
      "        \n",
      "        # Retrieve the architecture of the model to load, including \n",
      "        # inputs and outputs and stores these on the parameter server\n",
      "        self.get_network_info()\n",
      "        \n",
      "        # Retrieves the image shapes for the input and output from the\n",
      "        # now loaded model and stores these on the parameter server\n",
      "        self.get_model_info()\n",
      "\n",
      "\n",
      "    def setup_parameters(self, model_name: str, model_tag: str):\n",
      "    \n",
      "        # Instantiate our logger tool naming these processes and\n",
      "        # setting the tag. The (\"XX.\") convention indicates this is a \n",
      "        # model and log messages are coming from within the \n",
      "        # model processing script and not the main node.\n",
      "        self.logger = Logger(\n",
      "            name = model_name, \n",
      "            tag = model_tag)\n",
      "        \n",
      "        # Instantiate our timer tool which will output the times of\n",
      "        # the processes within the model, and indicate that the \n",
      "        # process originated from within the model, and not the module.\n",
      "        self.timer = ProcessTimer(logger=self.logger)\n",
      "        \n",
      "        # Creates a parameter manager\n",
      "        self.NeuralNetworkParams = Parameters(logger=self.logger)\n",
      "        \n",
      "    def setup_inference_engine(self, ie: IECore, viper_model: ViperModel, device: str):\n",
      "\n",
      "        # Link the internal inference engine with the initialized engine\n",
      "        # and read the network architecture.\n",
      "        self._ie = ie\n",
      "        \n",
      "        # Load the Viper Model class object, which contains the address\n",
      "        # for the neural network architecture and well as the weights\n",
      "        # of the trained model.\n",
      "        self._net = ie.read_network(\n",
      "            model=viper_model.location,\n",
      "            weights=viper_model.weights\n",
      "            )\n",
      "\n",
      "    def load_inference_engine(self, device, ie):\n",
      "        \n",
      "        # Load the network architecture and weights into the initialized\n",
      "        # inference engine. We must indicate the device name which \n",
      "        # is passed through the main node.\n",
      "        self._exec_net = ie.load_network(\n",
      "            network = self._net,\n",
      "            device_name = device\n",
      "            )\n",
      "            \n",
      "    def get_network_info(self):\n",
      "        \n",
      "        # Set the input and output blobs\n",
      "        self._input_blob = next(iter(self._exec_net.input_info))\n",
      "        self._output_blob = next(iter(self._exec_net.outputs))\n",
      "\n",
      "        # Get the input shape\n",
      "        #self._input_shape = self._net.inputs[self._input_blob].shape\n",
      "        #self.logger.i(f'Input shape: {self._input_shape}')\n",
      "        \n",
      "        # Save these parameters to the parameter server\n",
      "        #self.NeuralNetworkParams.add(\n",
      "        #    Parameter(\n",
      "        #        name = \"Input_shape\",\n",
      "        #        value = self._input_shape,\n",
      "        #        dynamic = False))\n",
      "            \n",
      "        # Get the output shape\n",
      "        self._output_shape = self._net.outputs[self._output_blob].shape\n",
      "        self.logger.i(f'Output shape: {self._output_shape}')\n",
      "        \n",
      "        # Save these parameters to the parameter server\n",
      "        self.NeuralNetworkParams.add(\n",
      "            Parameter(\n",
      "                name=\"Output_shape\",\n",
      "                value=self._output_shape,\n",
      "                dynamic=False))\n",
      "                \n",
      "    def get_model_info(self):\n",
      "        \n",
      "        # Accesses the shape of the input layer and the output layer\n",
      "        self._input_key = list(self._exec_net.input_info)[0]\n",
      "        self._output_keys = list(self._exec_net.outputs.keys())\n",
      "        self._tensors = self._exec_net.input_info[self._input_key].tensor_desc\n",
      "        \n",
      "        # Saves the shapes to variables representing\n",
      "        self.n, self.c, self.h, self.w = self._tensors.dims\n",
      "        self.logger.i(f'Tensor shape (NCHW): ({self.n}, {self.c}, {self.h}, {self.w})')\n",
      "        \n",
      "        self.NeuralNetworkParams.add(\n",
      "            Parameter(\n",
      "                name=\"Input_height\",\n",
      "                value=self.h,\n",
      "                dynamic=False))\n",
      "\n",
      "        self.NeuralNetworkParams.add(\n",
      "            Parameter(\n",
      "                name=\"Input_width\",\n",
      "                value=self.w,\n",
      "                dynamic=False))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from model_server import NeuralNetworkLoader\n",
    "lines = inspect.getsource(NeuralNetworkLoader)\n",
    "print(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pose Detection Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "from viper_toolkit import Dissect\n",
    "from pose_detection_module import PoseDetectionModel\n",
    "lines = inspect.getsource(PoseDetectionModel)\n",
    "print(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class PoseDetectionModel(NeuralNetworkLoader):\n",
      "    \"\"\"\n",
      "    A PoseDetectionModel is a class object of the type NeuralNetworkLoader\n",
      "    which connects this model to the parent inference engine, communicates\n",
      "    the neural network shape, and sets up logging.\n",
      "    \"\"\"\n",
      "    def __init__(self, *args, **kwargs):\n",
      "        super().__init__(*args, **kwargs)\n",
      "        self.load_decoder()\n",
      "        \n",
      "    def load_decoder (self):\n",
      "        # Load the model decoder which will decode the results of our \n",
      "        # ANN into nodes and edges representing a human being.\n",
      "        self.decoder = OpenPoseDecoder()\n",
      "        #self.logger.i(\"Decoder loaded\")\n",
      "        \n",
      "    def process_results(self, frame, results):\n",
      "        # the process_results function takes an image, the results inference\n",
      "        # the output_keys of the model, the inference engine, \n",
      "        # and a decoder which contains the shape of the entity (i.e., human)\n",
      "        # and gets poses from results\n",
      "        #self.timer.lap(\"Processing Results\")\n",
      "        rospy.loginfo(\"process results\")\n",
      "        pafs = results[self._output_keys[0]]\n",
      "        #print (\"PAFS\")\n",
      "        #print(pafs)\n",
      "        heatmaps = results[self._output_keys[1]]\n",
      "        #print(\"HEATMAPS\")\n",
      "        #print(heatmaps)\n",
      "        \n",
      "        # The heatmaps are generated from the image by using Max pooling,\n",
      "        # which takes the maximum value of the pixels contained within \n",
      "        # the shape of the kernel.\n",
      "        # https://github.com/openvinotoolkit/open_model_zoo/blob/master/demos/common/python/models/open_pose.py\n",
      "        rospy.loginfo(\"pool heatmaps\")\n",
      "        pooled_heatmaps = np.array(\n",
      "            [[pool2d(h, \n",
      "                    kernel_size=3, \n",
      "                    stride=1, \n",
      "                    padding=1, \n",
      "                    pool_mode=\"max\"\n",
      "                    ) for h in heatmaps[0]]]\n",
      "                    )\n",
      "        print (pooled_heatmaps)\n",
      "        nms_heatmaps = heatmap_nms(heatmaps, pooled_heatmaps)\n",
      "        print(\"NMS HEATMAPS ===========================\")\n",
      "        print(nms_heatmaps)\n",
      "        #self.timer.time(\"Processing Results\")\n",
      "        #self.timer.lap(\"Decoding Poses\")\n",
      "        rospy.loginfo(\"decode\")\n",
      "        # The generated heatmaps are then sent to the decoder\n",
      "        \n",
      "        poses, scores = self.decoder(\n",
      "            heatmaps, \n",
      "            nms_heatmaps, \n",
      "            pafs\n",
      "        )\n",
      "        print(\"post decode\")\n",
      "        print(poses)\n",
      "        print(scores)\n",
      "        \n",
      "        output_shape = self._exec_net.outputs[self._output_keys[0]].shape\n",
      "        print(\"-------------outputshape\")\n",
      "        print (output_shape)\n",
      "        \n",
      "        output_scale = frame.shape[1] / output_shape[3], frame.shape[0] / output_shape[2]\n",
      "        print(\"---------output scale\")\n",
      "        print(output_scale)\n",
      "        #rospy.loginfo(f\"Frame width, height: ({frame.shape[0]}, {frame.shape[1]})\")\n",
      "        #self.logger.i(f\"Output width, height: ({output_shape[2]}, {output_shape[3]})\")\n",
      "        #self.logger.i(f\"Output_scale: {output_scale}\")\n",
      "        \n",
      "        # multiply coordinates by scaling factor\n",
      "        poses[:, :, :2] *= output_scale\n",
      "        #self.timer(\"Decoding Poses\")\n",
      "        return poses, scores\n",
      "\n",
      "\n",
      "    def run_pose_estimation(self, frame):\n",
      "        # Model Preperation\n",
      "        # Initializing my timer module which will calcualtes the processing time\n",
      "        # between any two 'tags'\n",
      "        #self.timer =  ProcessTimer(logger = self.logger)\n",
      "        #self.timer.lap(\"Image Processing\")\n",
      "        \n",
      "        # Scaling the model\n",
      "        scale = 1280 / max(frame.shape)\n",
      "        #rospy.loginfo(\"scale: %s\", scale)\n",
      "        rospy.loginfo(\"init\")\n",
      "        if scale < 1:\n",
      "            frame = cv2.resize(\n",
      "                frame, \n",
      "                None, \n",
      "                fx=scale, \n",
      "                fy=scale, \n",
      "                interpolation=cv2.INTER_AREA\n",
      "                )\n",
      "        \n",
      "        # resize image and change dims to fit neural network input\n",
      "        # (see https://github.com/openvinotoolkit/open_model_zoo/\n",
      "        #  tree/master/models/intel/human-pose-estimation-0001)\n",
      "        input_img = cv2.resize(\n",
      "            frame, \n",
      "            (self.w, self.h), \n",
      "            interpolation=cv2.INTER_AREA\n",
      "            )\n",
      "        rospy.loginfo(\"resized\")\n",
      "        #self.logger.i(f\"Input Shape: {input_img.shape}\")\n",
      "        \n",
      "        # create batch of images (size = 1)\n",
      "        input_img = input_img.transpose(2, 0, 1)[np.newaxis, ...]\n",
      "        rospy.loginfo(\"transposed\")\n",
      "        #self.logger.i(f\"Transposed Shape: {input_img.shape}\")\n",
      "        \n",
      "        # Printing Image Processing Time\n",
      "        #self.timer.time(\"Image Processing\")\n",
      "        \n",
      "        # Performing Model Inference\n",
      "        # Performing inference and receiving the results.\n",
      "        #self.timer.lap(\"Inference\")\n",
      "        results = self._exec_net.infer(\n",
      "            inputs={self._input_key: input_img})\n",
      "        rospy.loginfo(\"infer\")\n",
      "        #self.logger.i(f\"Results Shape: {results.shape}\")\n",
      "        #self.timer.time(\"Inference\")\n",
      "        print(results)\n",
      "        # get poses from network results\n",
      "        poses, scores = self.process_results(frame=frame, results=results)\n",
      "        #self.logger.i(f\"Returned Shape: {poses.shape}\")\n",
      "        print (\"------------------------------------\")\n",
      "        print (scores)\n",
      "        print (poses)\n",
      "        # Outputing time of tag 'total_runtime' which was created upon\n",
      "        # timer initialization.)\n",
      "        #self.timer.time(total_runtime)\n",
      "        \n",
      "        return poses #, scores\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lines = inspect.getsource(PoseDetectionModel)\n",
    "print(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

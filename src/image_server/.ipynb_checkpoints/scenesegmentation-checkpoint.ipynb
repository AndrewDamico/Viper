{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "from argparse import ArgumentParser, SUPPRESS\n",
    "from pathlib import Path\n",
    "from time import perf_counter\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from openvino.inference_engine import IECore\n",
    "\n",
    "from src.scene_segmentation_model.async_pipeline import get_user_config, AsyncPipeline\n",
    "from src.scene_segmentation_model.performance_metrics import PerformanceMetrics\n",
    "from src.scene_segmentation_model.helpers import resolution\n",
    "from src.scene_segmentation_model.images_capture import open_images_capture\n",
    "from src.scene_segmentation_model.segmentation import SegmentationModel, SalientObjectDetectionModel\n",
    "from src.scene_segmentation_model.utils import OutputTransform\n",
    "\n",
    "\n",
    "logging.basicConfig(format='[ %(levelname)s ] %(message)s', level=logging.INFO, stream=sys.stdout)\n",
    "log = logging.getLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationVisualizer:\n",
    "    pascal_voc_palette = [\n",
    "        (0,   0,   0),\n",
    "        (128, 0,   0),\n",
    "        (0,   128, 0),\n",
    "        (128, 128, 0),\n",
    "        (0,   0,   128),\n",
    "        (128, 0,   128),\n",
    "        (0,   128, 128),\n",
    "        (128, 128, 128),\n",
    "        (64,  0,   0),\n",
    "        (192, 0,   0),\n",
    "        (64,  128, 0),\n",
    "        (192, 128, 0),\n",
    "        (64,  0,   128),\n",
    "        (192, 0,   128),\n",
    "        (64,  128, 128),\n",
    "        (192, 128, 128),\n",
    "        (0,   64,  0),\n",
    "        (128, 64,  0),\n",
    "        (0,   192, 0),\n",
    "        (128, 192, 0),\n",
    "        (0,   64,  128)\n",
    "    ]\n",
    "\n",
    "    def __init__(self, colors_path=None):\n",
    "        if colors_path:\n",
    "            self.color_palette = self.get_palette_from_file(colors_path)\n",
    "        else:\n",
    "            self.color_palette = self.pascal_voc_palette\n",
    "        self.color_map = self.create_color_map()\n",
    "\n",
    "    def get_palette_from_file(self, colors_path):\n",
    "        with open(colors_path, 'r') as file:\n",
    "            colors = []\n",
    "            for line in file.readlines():\n",
    "                values = line[line.index('(')+1:line.index(')')].split(',')\n",
    "                colors.append([int(v.strip()) for v in values])\n",
    "            return colors\n",
    "\n",
    "    def create_color_map(self):\n",
    "        classes = np.array(self.color_palette, dtype=np.uint8)[:, ::-1] # RGB to BGR\n",
    "        color_map = np.zeros((256, 1, 3), dtype=np.uint8)\n",
    "        classes_num = len(classes)\n",
    "        color_map[:classes_num, 0, :] = classes\n",
    "        color_map[classes_num:, 0, :] = np.random.uniform(0, 255, size=(256-classes_num, 3))\n",
    "        return color_map\n",
    "\n",
    "    def apply_color_map(self, input):\n",
    "        input_3d = cv2.merge([input, input, input])\n",
    "        return cv2.LUT(input_3d, self.color_map)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaliencyMapVisualizer:\n",
    "    def apply_color_map(self, input):\n",
    "        saliency_map = (input * 255.0).astype(np.uint8)\n",
    "        saliency_map = cv2.merge([saliency_map, saliency_map, saliency_map])\n",
    "        return saliency_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_segmentation(frame, masks, visualiser, resizer, only_masks=False):\n",
    "    output = visualiser.apply_color_map(masks)\n",
    "    if not only_masks:\n",
    "        output = np.floor_divide(frame, 2) + np.floor_divide(output, 2)\n",
    "    return resizer.resize(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_argparser():\n",
    "    parser = ArgumentParser(add_help=False)\n",
    "    args = parser.add_argument_group('Options')\n",
    "    args.add_argument('-h', '--help', action='help', default=SUPPRESS, help='Show this help message and exit.')\n",
    "    args.add_argument('-m', '--model', help='Required. Path to an .xml file with a trained model.',\n",
    "                      required=True, type=Path)\n",
    "    args.add_argument('-at', '--architecture_type', help='Required. Specify the model\\'s architecture type.',\n",
    "                      type=str, required=False, default='segmentation', choices=('segmentation', 'salient_object_detection'))\n",
    "    args.add_argument('-i', '--input', required=True,\n",
    "                      help='Required. An input to process. The input must be a single image, '\n",
    "                           'a folder of images, video file or camera id.')\n",
    "    args.add_argument('-d', '--device', default='CPU', type=str,\n",
    "                      help='Optional. Specify the target device to infer on; CPU, GPU, HDDL or MYRIAD is '\n",
    "                           'acceptable. The demo will look for a suitable plugin for device specified. '\n",
    "                           'Default value is CPU.')\n",
    "\n",
    "    common_model_args = parser.add_argument_group('Common model options')\n",
    "    common_model_args.add_argument('-c', '--colors', type=Path,\n",
    "                                   help='Optional. Path to a text file containing colors for classes.')\n",
    "\n",
    "    infer_args = parser.add_argument_group('Inference options')\n",
    "    infer_args.add_argument('-nireq', '--num_infer_requests', help='Optional. Number of infer requests.',\n",
    "                            default=1, type=int)\n",
    "    infer_args.add_argument('-nstreams', '--num_streams',\n",
    "                            help='Optional. Number of streams to use for inference on the CPU or/and GPU in throughput '\n",
    "                                 'mode (for HETERO and MULTI device cases use format '\n",
    "                                 '<device1>:<nstreams1>,<device2>:<nstreams2> or just <nstreams>).',\n",
    "                            default='', type=str)\n",
    "    infer_args.add_argument('-nthreads', '--num_threads', default=None, type=int,\n",
    "                            help='Optional. Number of threads to use for inference on CPU (including HETERO cases).')\n",
    "\n",
    "    io_args = parser.add_argument_group('Input/output options')\n",
    "    io_args.add_argument('--loop', default=False, action='store_true',\n",
    "                         help='Optional. Enable reading the input in a loop.')\n",
    "    io_args.add_argument('-o', '--output', required=False,\n",
    "                         help='Optional. Name of the output file(s) to save.')\n",
    "    io_args.add_argument('-limit', '--output_limit', required=False, default=1000, type=int,\n",
    "                         help='Optional. Number of frames to store in output. '\n",
    "                              'If 0 is set, all frames are stored.')\n",
    "    io_args.add_argument('--no_show', help=\"Optional. Don't show output.\", action='store_true')\n",
    "    io_args.add_argument('--output_resolution', default=None, type=resolution,\n",
    "                         help='Optional. Specify the maximum output window resolution '\n",
    "                              'in (width x height) format. Example: 1280x720. '\n",
    "                              'Input frame size used by default.')\n",
    "    io_args.add_argument('-u', '--utilization_monitors', default='', type=str,\n",
    "                         help='Optional. List of monitors to show initially.')\n",
    "    io_args.add_argument('--only_masks', default=False, action='store_true',\n",
    "                         help='Optional. Display only masks. Could be switched by TAB key.')\n",
    "    return parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(ie, args):\n",
    "    if args.architecture_type == 'segmentation':\n",
    "        return SegmentationModel(ie, args.model), SegmentationVisualizer(args.colors)\n",
    "    if args.architecture_type == 'salient_object_detection':\n",
    "        return SalientObjectDetectionModel(ie, args.model), SaliencyMapVisualizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    metrics = PerformanceMetrics()\n",
    "    rgs = build_argparser().parse_args()\n",
    "\n",
    "    log.info('Initializing Inference Engine...')\n",
    "    ie = IECore()\n",
    "\n",
    "    plugin_config = get_user_config(args.device, args.num_streams, args.num_threads)\n",
    "\n",
    "    log.info('Loading network...')\n",
    "\n",
    "    model, visualizer = get_model(ie, args)\n",
    "\n",
    "    pipeline = AsyncPipeline(ie, model, plugin_config, device=args.device, max_num_requests=args.num_infer_requests)\n",
    "\n",
    "    cap = open_images_capture(args.input, args.loop)\n",
    "\n",
    "    next_frame_id = 0\n",
    "    next_frame_id_to_show = 0\n",
    "\n",
    "    log.info('Starting inference...')\n",
    "    print(\"To close the application, press 'CTRL+C' here or switch to the output window and press ESC key\")\n",
    "\n",
    "    presenter = None\n",
    "    output_transform = None\n",
    "    video_writer = cv2.VideoWriter()\n",
    "    only_masks = args.only_masks\n",
    "    while True:\n",
    "        if pipeline.is_ready():\n",
    "            # Get new image/frame\n",
    "            start_time = perf_counter()\n",
    "            frame = cap.read()\n",
    "            if frame is None:\n",
    "                if next_frame_id == 0:\n",
    "                    raise ValueError(\"Can't read an image from the input\")\n",
    "                break\n",
    "            if next_frame_id == 0:\n",
    "                output_transform = OutputTransform(frame.shape[:2], args.output_resolution)\n",
    "                if args.output_resolution:\n",
    "                    output_resolution = output_transform.new_resolution\n",
    "                else:\n",
    "                    output_resolution = (frame.shape[1], frame.shape[0])\n",
    "                presenter = monitors.Presenter(args.utilization_monitors, 55,\n",
    "                                               (round(output_resolution[0] / 4), round(output_resolution[1] / 8)))\n",
    "                if args.output and not video_writer.open(args.output, cv2.VideoWriter_fourcc(*'MJPG'),\n",
    "                                                         cap.fps(), output_resolution):\n",
    "                    raise RuntimeError(\"Can't open video writer\")\n",
    "            # Submit for inference\n",
    "            pipeline.submit_data(frame, next_frame_id, {'frame': frame, 'start_time': start_time})\n",
    "            next_frame_id += 1\n",
    "        else:\n",
    "            # Wait for empty request\n",
    "            pipeline.await_any()\n",
    "\n",
    "        if pipeline.callback_exceptions:\n",
    "            raise pipeline.callback_exceptions[0]\n",
    "        # Process all completed requests\n",
    "        results = pipeline.get_result(next_frame_id_to_show)\n",
    "        if results:\n",
    "            objects, frame_meta = results\n",
    "            frame = frame_meta['frame']\n",
    "            start_time = frame_meta['start_time']\n",
    "            frame = render_segmentation(frame, objects, visualizer, output_transform, only_masks)\n",
    "            presenter.drawGraphs(frame)\n",
    "            metrics.update(start_time, frame)\n",
    "\n",
    "            if video_writer.isOpened() and (args.output_limit <= 0 or next_frame_id_to_show <= args.output_limit-1):\n",
    "                video_writer.write(frame)\n",
    "            next_frame_id_to_show += 1\n",
    "\n",
    "            if not args.no_show:\n",
    "                cv2.imshow('Segmentation Results', frame)\n",
    "                key = cv2.waitKey(1)\n",
    "                if key == 27 or key == 'q' or key == 'Q':\n",
    "                    break\n",
    "                if key == 9:\n",
    "                    only_masks = not only_masks\n",
    "                presenter.handleKey(key)\n",
    "\n",
    "    pipeline.await_all()\n",
    "    # Process completed requests\n",
    "    for next_frame_id_to_show in range(next_frame_id_to_show, next_frame_id):\n",
    "        results = pipeline.get_result(next_frame_id_to_show)\n",
    "        while results is None:\n",
    "            results = pipeline.get_result(next_frame_id_to_show)\n",
    "        objects, frame_meta = results\n",
    "        frame = frame_meta['frame']\n",
    "        start_time = frame_meta['start_time']\n",
    "\n",
    "        frame = render_segmentation(frame, objects, visualizer, output_transform, only_masks)\n",
    "        presenter.drawGraphs(frame)\n",
    "        metrics.update(start_time, frame)\n",
    "\n",
    "        if video_writer.isOpened() and (args.output_limit <= 0 or next_frame_id_to_show <= args.output_limit-1):\n",
    "            video_writer.write(frame)\n",
    "\n",
    "        if not args.no_show:\n",
    "            cv2.imshow('Segmentation Results', frame)\n",
    "            key = cv2.waitKey(1)\n",
    "\n",
    "    metrics.print_total()\n",
    "    print(presenter.reportMeans())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    sys.exit(main() or 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

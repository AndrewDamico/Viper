{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Intel Neural Compute Stick 2 is a compact and yet powerful Vision Processing Unit (VPU) which is optimized to perform the complex matrix calculations required for convolutional neural networks. Similar to a GPU, the VPU is engineered specifically for computer vision inference on images and videos.\n",
    "\n",
    "The VPU is initalized with the network structure of the neural networks as well as the weights; these networks are then allocated space on the device. Total time to initialize a network is approximately 30 seconds, however once it has been initalized successive inference on a loaded model is processed in milliseconds. \n",
    "\n",
    "In order for multiple modules to make use of the VPU, it is therefor desirable for the device to be initialized once with multiple models. \n",
    "\n",
    "To accomplish this I have engineered the Model Server. The Model Server is responsable for initializing the VPU device, and registering the various Networks with the device. The Model Server then makes an Inference Service API available to the modules. If a module needs inference performed, it uses the Model Server API to provide the server with the kind of model needed, as well as the image requirements. The model server then uses its most recent image and performs inference on the VPU, returning the results to the model module where additional  processing can take place.\n",
    "\n",
    "In order to receive the most recent images, the Modeling Server offers a second API endpoint for the Image Server Module. A limitation of ROS Noetic is that a service node cannot subscribe to other nodes like normal nodes can; this is because the service node needs to continually monitor the API requests. As such, the Model Server cannot obtain images on its own outside of an API call. In order to minimize the bandwidth and the duplication of images, the Image Server offers a kind of \"reverse\" service. Rather than acting as an API endpoint, the Image Server makes a service call to the Model Server requesting to give it an image via the Image Fetching Service offered by the model server. The Model Server then accepts that image, and the Image Server carries on with the management of the Image flags and requests for the nodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from viper_toolkit import Dissect\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model Server Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'ModelServer' from 'model_server' (/home/andrew/viper/devel/lib/python3/dist-packages/model_server/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_101336/79211271.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscripts\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodel_server\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmodel_server\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModelServer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0msource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetsource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModelServer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'ModelServer' from 'model_server' (/home/andrew/viper/devel/lib/python3/dist-packages/model_server/__init__.py)"
     ]
    }
   ],
   "source": [
    "from scripts import model_server\n",
    "from model_server import ModelServer\n",
    "source = inspect.getsource(ModelServer)\n",
    "print (source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class NeuralNetworkLoader(object):\n",
      "    \"\"\"\n",
      "    \n",
      "    A NeuralNetworkLoader is a class object which loads a pretrained\n",
      "    model (architecture and weights) onto a initialized OpenVino\n",
      "    inference engine.\n",
      "    \n",
      "    Keyword Arguments:\n",
      "    \n",
      "    ie -- an Inference Engine instance set up in the parent node which \n",
      "    we will load this model to.\n",
      "    \n",
      "    ViperModel -- a instance of class ViperModel which contains the \n",
      "    models weights and the structure of the neural network.\n",
      "    \n",
      "    device -- the inference device to be used to predict on (i.e., \n",
      "    \"MYRIAD\", CPU, GPU, etc.)\n",
      "    \n",
      "    model_tag -- a three letter abbreviation used by the VIPER Logger\n",
      "    module which identifies log messages as originating from within this\n",
      "    modules code.\n",
      "    \n",
      "    model_name -- a logger attribute which identifies this model.\n",
      "    \n",
      "    \"\"\"\n",
      "    \n",
      "    def __init__(self, \n",
      "                ie: IECore, \n",
      "                viper_model: ViperModel,\n",
      "                device: str,\n",
      "                model_tag: str = \"M..\",\n",
      "                model_name: str = \"Model\",\n",
      "                *args,\n",
      "                **kwargs):\n",
      "        \n",
      "        # Creates our helper tools from our Viper Toolkkit such as \n",
      "        # the parameter manager, our log manager, and our timer.\n",
      "        self.setup_parameters(\n",
      "            model_name = model_name, \n",
      "            model_tag = model_tag)\n",
      "        \n",
      "        # Prepare this model for loading\n",
      "        self.setup_inference_engine(\n",
      "            ie = ie, \n",
      "            viper_model = viper_model, \n",
      "            device = device)\n",
      "\n",
      "        # Load the read network onto the initialized device.\n",
      "        self.load_inference_engine(device=device, ie = ie)\n",
      "        \n",
      "        # Retrieve the architecture of the model to load, including \n",
      "        # inputs and outputs and stores these on the parameter server\n",
      "        self.get_network_info()\n",
      "        \n",
      "        # Retrieves the image shapes for the input and output from the\n",
      "        # now loaded model and stores these on the parameter server\n",
      "        self.get_model_info()\n",
      "\n",
      "\n",
      "    def setup_parameters(self, model_name: str, model_tag: str):\n",
      "    \n",
      "        # Instantiate our logger tool naming these processes and\n",
      "        # setting the tag. The (\"XX.\") convention indicates this is a \n",
      "        # model and log messages are coming from within the \n",
      "        # model processing script and not the main node.\n",
      "        self.logger = Logger(\n",
      "            name = model_name, \n",
      "            tag = model_tag)\n",
      "        \n",
      "        # Instantiate our timer tool which will output the times of\n",
      "        # the processes within the model, and indicate that the \n",
      "        # process originated from within the model, and not the module.\n",
      "        self.timer = ProcessTimer(logger=self.logger)\n",
      "        \n",
      "        # Creates a parameter manager\n",
      "        self.NeuralNetworkParams = Parameters(logger=self.logger)\n",
      "        \n",
      "    def setup_inference_engine(self, ie: IECore, viper_model: ViperModel, device: str):\n",
      "\n",
      "        # Link the internal inference engine with the initialized engine\n",
      "        # and read the network architecture.\n",
      "        self._ie = ie\n",
      "        \n",
      "        # Load the Viper Model class object, which contains the address\n",
      "        # for the neural network architecture and well as the weights\n",
      "        # of the trained model.\n",
      "        self._net = ie.read_network(\n",
      "            model=viper_model.location,\n",
      "            weights=viper_model.weights\n",
      "            )\n",
      "\n",
      "    def load_inference_engine(self, device, ie):\n",
      "        \n",
      "        # Load the network architecture and weights into the initialized\n",
      "        # inference engine. We must indicate the device name which \n",
      "        # is passed through the main node.\n",
      "        self._exec_net = ie.load_network(\n",
      "            network = self._net,\n",
      "            device_name = device\n",
      "            )\n",
      "            \n",
      "    def get_network_info(self):\n",
      "        \n",
      "        # Set the input and output blobs\n",
      "        self._input_blob = next(iter(self._exec_net.input_info))\n",
      "        self._output_blob = next(iter(self._exec_net.outputs))\n",
      "\n",
      "        # Get the input shape\n",
      "        #self._input_shape = self._net.inputs[self._input_blob].shape\n",
      "        #self.logger.i(f'Input shape: {self._input_shape}')\n",
      "        \n",
      "        # Save these parameters to the parameter server\n",
      "        #self.NeuralNetworkParams.add(\n",
      "        #    Parameter(\n",
      "        #        name = \"Input_shape\",\n",
      "        #        value = self._input_shape,\n",
      "        #        dynamic = False))\n",
      "            \n",
      "        # Get the output shape\n",
      "        self._output_shape = self._net.outputs[self._output_blob].shape\n",
      "        self.logger.i(f'Output shape: {self._output_shape}')\n",
      "        \n",
      "        # Save these parameters to the parameter server\n",
      "        self.NeuralNetworkParams.add(\n",
      "            Parameter(\n",
      "                name=\"Output_shape\",\n",
      "                value=self._output_shape,\n",
      "                dynamic=False))\n",
      "                \n",
      "    def get_model_info(self):\n",
      "        \n",
      "        # Accesses the shape of the input layer and the output layer\n",
      "        self._input_key = list(self._exec_net.input_info)[0]\n",
      "        self._output_keys = list(self._exec_net.outputs.keys())\n",
      "        self._tensors = self._exec_net.input_info[self._input_key].tensor_desc\n",
      "        \n",
      "        # Saves the shapes to variables representing\n",
      "        self.n, self.c, self.h, self.w = self._tensors.dims\n",
      "        self.logger.i(f'Tensor shape (NCHW): ({self.n}, {self.c}, {self.h}, {self.w})')\n",
      "        \n",
      "        self.NeuralNetworkParams.add(\n",
      "            Parameter(\n",
      "                name=\"Input_height\",\n",
      "                value=self.h,\n",
      "                dynamic=False))\n",
      "\n",
      "        self.NeuralNetworkParams.add(\n",
      "            Parameter(\n",
      "                name=\"Input_width\",\n",
      "                value=self.w,\n",
      "                dynamic=False))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from model_server import NeuralNetworkLoader\n",
    "source = inspect.getsource(NeuralNetworkLoader)\n",
    "print (source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class ViperModel(object):\n",
      "    # A ViperModel contains the location of the model architecture\n",
      "    # and the model weights which are stored in the node package.\n",
      "    \n",
      "    def __init__(self, package_name, model_xml, weights_bin):\n",
      "        self.pkg = package_name\n",
      "        self.model = model_xml\n",
      "        self.weights = weights_bin\n",
      "        self.setup_model()\n",
      "        \n",
      "    def setup_model(self):\n",
      "        self.dir = roslib.packages.get_pkg_dir(self.pkg)\n",
      "        self.location = os.path.join(\n",
      "            self.dir, \n",
      "            self.model\n",
      "            )\n",
      "        self.weights = os.path.join(\n",
      "            self.dir, \n",
      "            self.weights\n",
      "            )\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from model_server import ViperModel\n",
    "source = inspect.getsource(ViperModel)\n",
    "print (source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_server.srv import ImageRequest, ImageRequestResponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = ImageRequest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "status: \n",
       "  data: False"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ImageRequestResponse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_server.msg import InferenceResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "structure: []\n",
      "inferences: []\n",
      "Structure Datatype: <class 'list'>\n",
      "Inferences Datatype: <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(InferenceResults())\n",
    "print (f'Structure Datatype: {type(InferenceResults().structure)}')\n",
    "print (f'Inferences Datatype: {type(InferenceResults().inferences)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
